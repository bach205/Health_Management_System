{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doi output het roi moi tra ra response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_chatbot(question):\n",
    "  prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": question}],\n",
    "    tokenize=False\n",
    "  )\n",
    "\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "  outputs = model.generate(**inputs, max_new_tokens=1024, temperature=0.7)\n",
    "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stream output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def stream_chatbot(question):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": question}],\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    generated = input_ids\n",
    "    past_key_values = None\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "                input_ids=generated[:, :-1],\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        for _ in range(1024):  # max_new_tokens\n",
    "            outputs = model(\n",
    "                input_ids=generated[:, -1:],\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # In token mới ra ngay\n",
    "            new_text = tokenizer.decode(next_token[0],skip_special_tokens=True)\n",
    "\n",
    "            if(not is_answer):\n",
    "              if(new_text == \"\\n\\n\"):\n",
    "                is_answer = True\n",
    "            else:\n",
    "                print(new_text, end=\"\", flush=True)\n",
    "\n",
    "            # Kiểm tra nếu gặp <eos>\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Cập nhật\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            past_key_values = outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import StreamingResponse\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def event_stream():\n",
    "    text = [\"Xin\", \" chào\", \" bạn\", \" nhé\", \"!\"]\n",
    "    for word in text:\n",
    "        time.sleep(0.5)  # Giả lập độ trễ như GPT\n",
    "        yield f\"data: {{\" \\\n",
    "              f\"\\\"choices\\\": [{{\\\"delta\\\": {{\\\"content\\\": \\\"{word}\\\"}}}}]\" \\\n",
    "              f\"}}\\n\\n\"\n",
    "    # Kết thúc\n",
    "    yield \"data: {\\\"choices\\\":[{\\\"delta\\\":{},\\\"finish_reason\\\":\\\"stop\\\"}]}\\n\\n\"\n",
    "\n",
    "@app.post(\"/chat/completion\")\n",
    "def chat_completion():\n",
    "    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical RAG System\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) system for medical question answering, built with Python and FastAPI.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "HEALTH_CARE_AI/\n",
    "│\n",
    "├── src/                           # Source code\n",
    "│   ├── crawler/                   # Data crawling\n",
    "│   ├── rag_engine/               # RAG implementation\n",
    "│   ├── server/                   # FastAPI server\n",
    "│   └── client/                   # Client applications\n",
    "│\n",
    "├── data/                         # Data storage\n",
    "├── models/                       # Model storage\n",
    "├── vector_store/                # Vector database\n",
    "└── notebooks/                   # Jupyter notebooks\n",
    "└── docker/                     # docker\n",
    "└── documents/                   # Save documents uploaded from client\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "1. Create a virtual environment and install dependencies:\n",
    "Window\n",
    "```bash\n",
    "conda env create -f environment_window.yml\n",
    "```\n",
    "Linux\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "<!-- 2. Install dependencies:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "pip uninstall -y unstructured\n",
    "pip install unstructured\n",
    "```\n",
    "\n",
    "3. Set up environment variables:\n",
    "```bash\n",
    "# Edit .env with your configuration\n",
    "# Extend: you need to install torch if you don't have it yet (by default, torch is using cpu cause i don't have gpu in local)\n",
    "``` -->\n",
    "\n",
    "2. Set up dependency application to use Unstructure in local (linux) or you can google \"how to use unstructure in local\"\n",
    "if you want to use unstructure api, you need to \n",
    "```bash\n",
    "# !sudo apt update && sudo apt install -y \\\n",
    "#   libmagic-dev \\\n",
    "#   poppler-utils \\\n",
    "#   tesseract-ocr \\\n",
    "#   qpdf \\\n",
    "#   libreoffice \\\n",
    "#   pandoc\n",
    "```\n",
    "\n",
    "3.Set up chromaDB (create chromaDB's storage in local)\n",
    "```bash\n",
    "python setup.py\n",
    "```\n",
    "\n",
    "## Running the Application\n",
    "\n",
    "1. Start the FastAPI server:\n",
    "```bash\n",
    "python -m uvicorn src.server.main:app --host 0.0.0.0 --port 8000 --workers 1\n",
    "```\n",
    "\n",
    "2. Access the API documentation at `http://localhost:8000/docs`\n",
    "\n",
    "## Features\n",
    "\n",
    "- Medical data crawling from multiple sources\n",
    "- RAG-based question answering\n",
    "- RESTful API interface\n",
    "- Server sent events streaming\n",
    "- Vector store for efficient retrieval\n",
    "- Support for multiple medical websites"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
